import os
import boto3
from langchain_chroma import Chroma
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from datetime import datetime
from pathlib import Path
import logging
import tempfile
import sys

# --- ê²½ë¡œ ë¬¸ì œ í•´ê²° ---
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
# --- ê²½ë¡œ ë¬¸ì œ í•´ê²° ---

from data_preprocessing import process_pdf_to_markdown

# ======================== ë¡œê¹… ì„¤ì • ========================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ======================== í™˜ê²½ ë³€ìˆ˜ ë° ìƒìˆ˜ ì„¤ì • ========================
AWS_S3_BUCKET = os.getenv("AWS_S3_BUCKET", "your-s3-bucket-name")
AWS_REGION = os.getenv("AWS_REGION", "ap-northeast-2")
# Corrected DB Path to point to the project root
CHROMA_DB_PATH = os.getenv("CHROMA_DB_PATH", "../chroma_db") 
CHROMA_COLLECTION_NAME = os.getenv("CHROMA_COLLECTION_NAME", "kobaco_pdf_collection")

embedding_function = OpenAIEmbeddings(model="text-embedding-3-large")

# ======================== ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ========================
def get_chroma_collection():
    """ChromaDB í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³  ì»¬ë ‰ì…˜ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        # Using LangChain's Chroma wrapper directly
        vector_store = Chroma(
            embedding_function=embedding_function,
            collection_name=CHROMA_COLLECTION_NAME,
            persist_directory=CHROMA_DB_PATH, # Use the consistent path
        )
        logger.info(f"ChromaDB ì»¬ë ‰ì…˜ '{CHROMA_COLLECTION_NAME}'ì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return vector_store
    except Exception as e:
        logger.error(f"ChromaDB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        raise

def extract_filename_from_s3_key(s3_key: str) -> str:
    """
    S3 í‚¤ì—ì„œ íŒŒì¼ëª…ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        s3_key: S3 ê°ì²´ í‚¤ (ì˜ˆ: "kobaco_data/ë‚´ë¶€ë¬¸ì„œ/íˆ¬ì/ë³´ê³ ì„œ_2023.pdf")
    
    Returns:
        íŒŒì¼ëª… (ì˜ˆ: "ë³´ê³ ì„œ_2023.pdf")
    """
    return Path(s3_key).name

def build_s3_url(bucket_name: str, s3_key: str) -> str:
    """
    S3 ë²„í‚·ëª…ê³¼ í‚¤ë¡œ ì™„ì „í•œ S3 URLì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        bucket_name: S3 ë²„í‚·ëª…
        s3_key: S3 ê°ì²´ í‚¤
    
    Returns:
        ì™„ì „í•œ S3 URL (ì˜ˆ: "s3://bucket-name/path/to/file.pdf")
    """
    return f"s3://{bucket_name}/{s3_key}"

# ======================== S3 PDF ì²˜ë¦¬ ë° ì„ë² ë”© ========================
def process_s3_pdfs_to_chroma(bucket_name: str, collection):
    """
    S3 ë²„í‚·ì—ì„œ PDF íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ê³ , í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•œ í›„,
    Chunking ë° ì„ë² ë”©ì„ ê±°ì³ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤.
    
    Metadata êµ¬ì¡°:
    - source: íŒŒì¼ëª…ë§Œ (ì˜ˆ: "ë³´ê³ ì„œ_2023.pdf")
    - s3_path: ì™„ì „í•œ S3 URL (ì˜ˆ: "s3://bucket/path/file.pdf")
    - chunk_index: ì²­í¬ ë²ˆí˜¸
    - processed_at: ì²˜ë¦¬ ì‹œê°„
    """
    s3_client = boto3.client('s3', region_name=AWS_REGION)
    paginator = s3_client.get_paginator('list_objects_v2')
    
    try:
        s3_prefix = "kobaco_data/ë‚´ë¶€ë¬¸ì„œ/íˆ¬ì/"
        logger.info(f"S3 ê²½ë¡œ '{s3_prefix}'ì—ì„œ íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.")

        pages = paginator.paginate(Bucket=bucket_name, Prefix=s3_prefix)
        pdf_files = [obj['Key'] for page in pages for obj in page.get('Contents', []) if obj['Key'].lower().endswith('.pdf')]
        logger.info(f"S3 ë²„í‚· '{bucket_name}'ì˜ '{s3_prefix}' ê²½ë¡œì—ì„œ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"S3 ë²„í‚·ì—ì„œ íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        return

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
        is_separator_regex=False,
    )

    processed_at = datetime.now().isoformat()

    for s3_key in pdf_files:
        # íŒŒì¼ëª…ê³¼ S3 ê²½ë¡œ ë¶„ë¦¬
        filename = extract_filename_from_s3_key(s3_key)
        s3_full_path = build_s3_url(bucket_name, s3_key)
        
        logger.info(f"ğŸ“„ íŒŒì¼ëª…: {filename}")
        logger.info(f"ğŸ”— S3 ê²½ë¡œ: {s3_full_path}")
        
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_dir_path = Path(temp_dir)
            local_pdf_path = temp_dir_path / filename  # íŒŒì¼ëª…ë§Œ ì‚¬ìš©
            
            try:
                logger.info(f"'{filename}' ë‹¤ìš´ë¡œë“œ ì¤‘...")
                s3_client.download_file(bucket_name, s3_key, str(local_pdf_path))

                logger.info(f"'{filename}' ì²˜ë¦¬ ì¤‘...")
                processed_md_path = process_pdf_to_markdown(local_pdf_path, temp_dir_path)
                
                if not processed_md_path or not processed_md_path.exists():
                    logger.warning(f"'{filename}' ì²˜ë¦¬ í›„ ê²°ê³¼ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    continue

                full_text = processed_md_path.read_text(encoding='utf-8')
                if not full_text.strip():
                    logger.warning(f"'{filename}'ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    continue
                
                chunks = text_splitter.split_text(full_text)
                logger.info(f"'{filename}'ë¥¼ {len(chunks)}ê°œì˜ chunkë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

                # ğŸ“ ê°œì„ ëœ Metadata êµ¬ì¡°
                metadatas = []
                for i, chunk in enumerate(chunks):
                    metadata = {
                        "source": filename,           # ğŸ·ï¸ íŒŒì¼ëª…ë§Œ (ì˜ˆ: "ë³´ê³ ì„œ_2023.pdf")
                        "s3_path": s3_full_path,     # ğŸ”— ì™„ì „í•œ S3 URL
                        "chunk_index": i,            # ğŸ“Š ì²­í¬ ë²ˆí˜¸
                        "total_chunks": len(chunks), # ğŸ“Š ì „ì²´ ì²­í¬ ìˆ˜
                        "processed_at": processed_at, # â° ì²˜ë¦¬ ì‹œê°„
                        "file_size_bytes": os.path.getsize(local_pdf_path), # ğŸ“ íŒŒì¼ í¬ê¸°
                    }
                    metadatas.append(metadata)

                if chunks:
                    # ê³ ìœ í•œ ID ìƒì„± (íŒŒì¼ëª… ê¸°ë°˜ìœ¼ë¡œ ë” ê¹”ë”í•˜ê²Œ)
                    doc_ids = [f"{filename.replace('.', '_')}_chunk_{i}" for i in range(len(chunks))]
                    
                    collection.add_texts(
                        texts=chunks,
                        metadatas=metadatas,
                        ids=doc_ids
                    )
                    logger.info(f"âœ… '{filename}'ì˜ {len(chunks)}ê°œ chunkë¥¼ ChromaDBì— ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
                    
                    # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
                    logger.info(f"   ğŸ“‹ Sample metadata: {metadatas[0]}")

            except Exception as e:
                logger.error(f"âŒ '{filename}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)

def verify_stored_data(collection, sample_filename: str = None):
    """
    ì €ì¥ëœ ë°ì´í„°ì˜ metadata êµ¬ì¡°ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.
    
    Args:
        collection: ChromaDB ì»¬ë ‰ì…˜
        sample_filename: í™•ì¸í•  ìƒ˜í”Œ íŒŒì¼ëª… (Noneì´ë©´ ì²« ë²ˆì§¸ ë¬¸ì„œ í™•ì¸)
    """
    try:
        # ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ
        if sample_filename:
            results = collection.similarity_search(
                query="íˆ¬ì",  # ìƒ˜í”Œ ì¿¼ë¦¬
                filter={"source": sample_filename},
                k=1
            )
        else:
            results = collection.similarity_search(query="íˆ¬ì", k=1)
        
        if results:
            sample_doc = results[0]
            logger.info("ğŸ“Š ì €ì¥ëœ ë°ì´í„° ìƒ˜í”Œ:")
            logger.info(f"   ğŸ“„ Source: {sample_doc.metadata.get('source')}")
            logger.info(f"   ğŸ”— S3 Path: {sample_doc.metadata.get('s3_path')}")
            logger.info(f"   ğŸ“Š Chunk Index: {sample_doc.metadata.get('chunk_index')}")
            logger.info(f"   â° Processed At: {sample_doc.metadata.get('processed_at')}")
            logger.info(f"   ğŸ“ Content Preview: {sample_doc.page_content[:100]}...")
        else:
            logger.warning("âš ï¸  ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ======================== ë©”ì¸ ì‹¤í–‰ ========================
if __name__ == "__main__":
    if AWS_S3_BUCKET == "your-s3-bucket-name":
        logger.error("âŒ AWS_S3_BUCKET í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ ìŠ¤í¬ë¦½íŠ¸ ë‚´ì—ì„œ ì§ì ‘ ë²„í‚· ì´ë¦„ì„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
    else:
        try:
            chroma_collection = get_chroma_collection()
            
            logger.info("ğŸš€ S3 PDF ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            process_s3_pdfs_to_chroma(AWS_S3_BUCKET, chroma_collection)
            
            logger.info("ğŸ” ì²˜ë¦¬ëœ ë°ì´í„° ê²€ì¦ ì¤‘...")
            verify_stored_data(chroma_collection)
            
            logger.info("âœ… ëª¨ë“  PDF íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.critical(f"ğŸ’¥ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)