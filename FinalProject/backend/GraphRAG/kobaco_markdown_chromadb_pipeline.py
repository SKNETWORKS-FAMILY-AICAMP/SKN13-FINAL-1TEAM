import os
import re
import json
import pandas as pd # Keep for now, might remove later if not strictly needed
from typing import List, Dict
from collections import defaultdict
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

# BASE_DIR = r"C:\Users\jhwoo\Desktop\SKN_ws\project\SKN13-FINAL-1TEAM\í•œêµ­ë°©ì†¡ê´‘ê³ ì§„í¥ê³µì‚¬\ë‚´ë¶€ë¬¸ì„œ\ì¬ë¬´ì„±ê³¼\_markdown_output" # ì œê±° ë˜ëŠ” ì£¼ì„ ì²˜ë¦¬

# def collect_markdown_files(base_dir):
#     markdown_files = []
#     for root, _, files in os.walk(base_dir):
#         for f in files:
#             if f.lower().endswith(".md"):
#                 markdown_files.append(os.path.join(root, f))
#     return markdown_files

def clean_text(text: str) -> str:
    # Markdown specific cleaning can be added here if needed
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def main(documents_to_process: List[tuple[str, str]]): # (ë¡œì»¬_ê²½ë¡œ, S3_ê°ì²´_í‚¤) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ëŠ”ë‹¤
    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)

    # markdown_files = collect_markdown_files(BASE_DIR) # ì œê±° ë˜ëŠ” ì£¼ì„ ì²˜ë¦¬
    print(f"ì´ {len(documents_to_process)}ê°œì˜ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.") # ìƒˆë¡œìš´ ì¶œë ¥ë¬¸

    all_chunks: List[Document] = []

    for local_path, s3_object_key in documents_to_process: # ì œê³µëœ íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•œë‹¤
        with open(local_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        text = clean_text(text)
        title = os.path.basename(local_path).replace(".md", "")
        
        from datetime import datetime

        metadata = {
            "activated": True,
            "content_length": len(text),
            "doc_category": os.path.basename(os.path.dirname(local_path)), # ë¡œì»¬ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ ì¹´í…Œê³ ë¦¬ ì„¤ì •
            "file_type": "md",
            "page": 1,
            "reg_date": datetime.now().isoformat(),
            "source": local_path, # ë¡œì»¬ íŒŒì¼ì˜ ì „ì²´ ì ˆëŒ€ ê²½ë¡œ
            "subject": title,
            "version": "1.0",
            "s3_object_key": s3_object_key # ğŸ”¥ ì´ ì¤„ì„ ì¶”ê°€í•œë‹¤ ğŸ”¥
        }
        doc = Document(page_content=text, metadata=metadata)

        chunks = splitter.split_documents([doc])
        for i, chunk in enumerate(chunks):
            print(f"DEBUG: Chunk {i} from {local_path}: {chunk.page_content[:100].encode('utf-8', 'ignore').decode('utf-8')}...")
        all_chunks.extend(chunks)

    embedding = OpenAIEmbeddings(model="text-embedding-3-large")
    vectorstore = Chroma.from_documents(
        documents=all_chunks,
        embedding=embedding,
        persist_directory="./chroma/kobaco_markdown"
    )
    print("[SUCCESS] ë§ˆí¬ë‹¤ìš´ ê¸°ë°˜ ChromaDBì— ë²¡í„° ì €ì¥ ì™„ë£Œ")

if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì‹œ: ì´ ë¶€ë¶„ì€ ë„¤ë†ˆì´ S3ì—ì„œ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•œ ì‹¤ì œ ê²½ë¡œì™€ S3 í‚¤ë¡œ ì±„ì›Œì•¼ í•œë‹¤.
    # ì§€ê¸ˆì€ ì˜ˆì‹œë¥¼ ìœ„í•´ ë”ë¯¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œë‹¤.
    # ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œëŠ” S3ì—ì„œ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•œ í›„ ì´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì±„ì›Œì•¼ í•œë‹¤.
    
    # ë”ë¯¸ ì˜ˆì‹œ:
    # S3ì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì´ ì´ ê²½ë¡œì— ìˆë‹¤ê³  ê°€ì •í•œë‹¤.
    dummy_local_path = r"C:\Users\jhwoo\Desktop\SKN_ws\project\SKN13-FINAL-1TEAM\í•œêµ­ë°©ì†¡ê´‘ê³ ì§„í¥ê³µì‚¬\ë‚´ë¶€ë¬¸ì„œ\ì¬ë¬´ì„±ê³¼\_markdown_output\example.md"
    dummy_s3_key = "í•œêµ­ë°©ì†¡ê´‘ê³ ì§„í¥ê³µì‚¬/ë‚´ë¶€ë¬¸ì„œ/ì¬ë¬´ì„±ê³¼/example.md" # ì˜ˆì‹œ S3 í‚¤

    # ëª¨ë“  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì— ëŒ€í•´ (ë¡œì»¬_ê²½ë¡œ, S3_ê°ì²´_í‚¤) ìŒìœ¼ë¡œ ì´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì±„ì›Œì•¼ í•œë‹¤.
    example_documents = [
        (dummy_local_path, dummy_s3_key),
        # ì—¬ê¸°ì— ë‹¤ë¥¸ (ë¡œì»¬_ê²½ë¡œ, S3_ê°ì²´_í‚¤) ìŒì„ ì¶”ê°€í•´ë¼
    ]
    main(example_documents)