# DocumentEditAgent.py

from typing import Any, List
from dotenv import load_dotenv
import asyncio

from langchain_core.messages import SystemMessage, ToolMessage, HumanMessage
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableConfig

from langgraph.graph import StateGraph, END
from langgraph.prebuilt import tools_condition
from langgraph.checkpoint.memory import MemorySaver

from .DocumentSearchAgentTools.AgentState import AgentState
from .DocumentEditorAgentTools.editor_tool import edit_document
from .document_editor_system_prompt import EDITOR_SYSTEM_PROMPT

load_dotenv()

# --- Main Agent Node ---

async def agent_node(state: AgentState, llm_with_tools: Any) -> dict:
    """
    The main node for the DocumentEditorAgent.
    It invokes the LLM with the current state to decide on the next action.
    """
    print("--- DocumentEditorAgent: Agent Node Running ---")
    
    # The LLM's messages should only contain the system prompt and the latest user message.
    # The document content itself is not passed to this LLM.
    system_prompt = EDITOR_SYSTEM_PROMPT
    messages = [
        SystemMessage(content=system_prompt),
        state["messages"][-1] # Get the latest user message
    ]

    # Invoke the LLM to get the next tool call
    response = await llm_with_tools.ainvoke(messages)
    print(f"--- DocumentEditorAgent: LLM produced a tool call: {response.tool_calls} ---")
    return {"messages": [response]}


# --- Custom Tool Node ---

async def custom_tool_node(state: AgentState) -> dict:
    """
    This node executes the tool call generated by the agent LLM.
    It's responsible for fetching the document_content from the state
    and passing it to the tool.
    """
    print("--- DocumentEditorAgent: Custom Tool Node Running ---")
    tool_call = state["messages"][-1].tool_calls[0]
    tool_name = tool_call["name"]
    tool_args = tool_call["args"]
    
    if tool_name != "edit_document":
        raise ValueError(f"Unsupported tool: {tool_name}")

    # Get the document content from the state
    document_content = state.get("document_content")
    if not document_content:
        raise ValueError("Document content is missing from the state.")

    # Call the tool with the instruction from the LLM and content from the state
    edited_content = await asyncio.to_thread(
        edit_document.invoke, 
        {
            "instruction": tool_args["instruction"],
            "document_content": document_content
        }
    )

    # Return the result as a ToolMessage to be processed by the state updater
    return {"messages": [ToolMessage(content=edited_content, tool_call_id=tool_call["id"])]}


# --- State Update Node ---

async def update_document_state(state: AgentState) -> dict:
    """
    After the tool node runs, this updates the state with the modified document.
    """
    print("--- DocumentEditorAgent: Updating document state ---")
    last_message = state["messages"][-1]
    if not isinstance(last_message, ToolMessage):
        return {}

    # Update document_content with the content of the last ToolMessage
    updated_content = last_message.content
    print(f"--- DocumentEditorAgent: New document content updated in state. ---")
    return {"document_content": updated_content, "messages": []} # Clear messages after edit


# --- Graph Factory ---

def DocumentEditAgent() -> Any:
    """Compiles and returns the LangGraph agent for document editing."""
    llm = ChatOpenAI(model_name='gpt-4o', temperature=0, streaming=True)
    
    # Define the single tool the agent will use
    tools = [edit_document]
    llm_with_tools = llm.bind_tools(tools)
    
    # Create the graph
    graph = StateGraph(AgentState)
    
    # Add nodes to the graph
    graph.add_node("agent", lambda state: agent_node(state, llm_with_tools))
    graph.add_node("tools", custom_tool_node)
    graph.add_node("update_state", update_document_state)
    
    # Define the graph's flow
    graph.set_entry_point("agent")
    graph.add_conditional_edges(
        "agent",
        tools_condition, # LangGraph's built-in conditional logic
        {END: END, "tools": "tools"}
    )
    graph.add_edge("tools", "update_state")
    graph.add_edge("update_state", END) # End the flow after updating the state
    
    # Compile the graph with memory
    return graph.compile(checkpointer=MemorySaver())


def generate_config(session_id: str) -> RunnableConfig:
    return RunnableConfig(
        recursion_limit=50,
        configurable={"thread_id": session_id},
    )