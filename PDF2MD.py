import fitz
import camelot
from pathlib import Path
from tqdm import tqdm
from PIL import Image
import pandas as pd
import logging
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from langchain_core.messages import HumanMessage
import base64


load_dotenv()
llm = ChatOpenAI(model_name="gpt-4o")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

def extract_text_from_page(page):
    """í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    text = page.get_text("text").strip()
    return ["### í…ìŠ¤íŠ¸ ë‚´ìš©", text] if text else []

def crop_table_from_page_image(page_image: Image.Image, table_bbox: list, page_width: float, page_height: float, padding: int = 10) -> Image.Image:
    """í˜ì´ì§€ ì´ë¯¸ì§€ì—ì„œ í‘œ ì˜ì—­ë§Œ í¬ë¡­ (ì—¬ë°± ì¶”ê°€)"""
    # camelot bbox: [x0, y0, x1, y1] (PDF ì¢Œí‘œê³„)
    x0, y0, x1, y1 = table_bbox
    
    # PDF ì¢Œí‘œê³„ë¥¼ ì´ë¯¸ì§€ ì¢Œí‘œê³„ë¡œ ë³€í™˜
    img_width, img_height = page_image.size
    scale_x = img_width / page_width
    scale_y = img_height / page_height
    
    left = int(x0 * scale_x) - padding
    top = int((page_height - y1) * scale_y) - padding  # PDFëŠ” bottom-up, ì´ë¯¸ì§€ëŠ” top-down
    right = int(x1 * scale_x) + padding
    bottom = int((page_height - y0) * scale_y) + padding
    
    # ê²½ê³„ê°’ ì²´í¬
    left = max(0, left)
    top = max(0, top)
    right = min(img_width, right)
    bottom = min(img_height, bottom)
    
    return page_image.crop((left, top, right, bottom))

def extract_tables_from_page(pdf_path: Path, page, page_num: int, tables_dir: Path, page_image: Image.Image):
    """í˜ì´ì§€ì—ì„œ í‘œ ì¶”ì¶œ ë° ë³„ë„ ì €ì¥"""
    md_chunks = []
    
    try:
        tables = camelot.read_pdf(str(pdf_path), pages=str(page_num + 1), flavor='lattice')
        if tables.n == 0:
            return md_chunks
            
        md_chunks.append("### ğŸ“Š í‘œ")

        # í† í¬ë‚˜ì´ì € ì„¸íŒ… (GPT-4 ê¸°ì¤€)
        # enc = tiktoken.encoding_for_model("gpt-4-vision-preview")

        for i, table in enumerate(tables):
            table_name = f"{pdf_path.stem}_p{page_num + 1}_table{i + 1}"
            csv_path = tables_dir / f"{table_name}.csv"
            img_path = None

            # 1. CSV ì €ì¥
            try:
                table.df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            except UnicodeEncodeError:
                table.df.to_csv(csv_path, index=False, encoding='cp949')

            # 2. í‘œ ì´ë¯¸ì§€ ì €ì¥
            try:
                if hasattr(table, '_bbox') and table._bbox:
                    table_bbox = table._bbox
                    page_width = page.rect.width
                    page_height = page.rect.height

                    cropped_table = crop_table_from_page_image(
                        page_image=page_image,
                        table_bbox=table_bbox,
                        page_width=page_width,
                        page_height=page_height,
                        padding=15
                    )
                    img_path = tables_dir / f"{table_name}.png"
                    cropped_table.save(img_path, 'PNG')
            except Exception as e:
                logger.error(f"í‘œ ì´ë¯¸ì§€ í¬ë¡­ ì‹¤íŒ¨ {pdf_path.name} p{page_num + 1} table{i + 1}: {e}")

            # 3. CSV í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ + í† í° ìˆ˜ ì œí•œ ì²˜ë¦¬
            csv_text = table.df.to_csv(index=False)
            # token_count = len(enc.encode(csv_text))
            # if token_count > max_csv_tokens:
            #     # í† í° ì œí•œ ì´ˆê³¼ ì‹œ ìë¥´ê¸° (ê°„ë‹¨í•˜ê²Œ ë¬¸ì ê¸°ì¤€)
            #     approx_limit = int(len(csv_text) * max_csv_tokens / token_count)
            #     csv_text = csv_text[:approx_limit] + "\n... (ì´í•˜ ìƒëµ)"

            # 4. ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ ìƒì„±
            content_blocks = [
                {"type": "text", "text": "ì´ í‘œë¥¼ ì„¤ëª…í•´ì¤˜. ì–´ë–¤ ë‚´ìš©ì„ ë‹´ê³  ìˆëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì¤˜."},
                {"type": "text", "text": f"í‘œì˜ ë°ì´í„° ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ë‹¤:\n{csv_text}"}
            ]

            if img_path and img_path.exists():
                with open(img_path, "rb") as f:
                    img_bytes = f.read()
                img_b64 = base64.b64encode(img_bytes).decode("utf-8")
                content_blocks.append({
                    "type": "image_url",
                    "image_url": {"url": f"data:image/png;base64,{img_b64}"}
                })

            messages = [HumanMessage(content=content_blocks)]

            # 5. ìº¡ì…˜ ìš”ì²­ ë° ë§ˆí¬ë‹¤ìš´ ì‘ì„±
            try:
                caption = llm.invoke(messages).content.strip()
            except Exception as e:
                logger.error(f"LLM ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨ {table_name}: {e}")
                caption = "âš ï¸ ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨"

            if img_path and img_path.exists():
                md_chunks.append(f"![í‘œ {i + 1}]({img_path.as_posix()})")
            md_chunks.append(f"- CSV íŒŒì¼: [{csv_path.name}]({csv_path.as_posix()})")
            md_chunks.append(f"**ìº¡ì…˜:** {caption}\n")

    except Exception as e:
        logger.error(f"í‘œ ì¶”ì¶œ ì‹¤íŒ¨ {pdf_path.name} p{page_num + 1}: {e}")
        
    return md_chunks

def extract_images_from_page(doc, page, page_num: int, pdf_stem: str, images_dir: Path):
    """í˜ì´ì§€ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ + Vision APIë¡œ ìº¡ì…˜ ìƒì„±"""
    md_chunks = []
    images = page.get_images(full=True)

    if not images:
        return md_chunks

    md_chunks.append("### ğŸ“¸ ì´ë¯¸ì§€")

    for idx, img in enumerate(images):
        try:
            xref = img[0]
            base = doc.extract_image(xref)
            ext = base["ext"]
            img_bytes = base["image"]

            filename = f"{pdf_stem}_p{page_num + 1}_img{idx + 1}.{ext}"
            img_path = images_dir / filename

            with open(img_path, "wb") as f:
                f.write(img_bytes)

            # MIME íƒ€ì… ê²°ì •
            mime = f"image/{'jpeg' if ext == 'jpg' else ext}"
            b64_img = base64.b64encode(img_bytes).decode("utf-8")

            # LangChain ë©”ì‹œì§€ êµ¬ì„±
            messages = [
                HumanMessage(
                    content=[
                        {"type": "text", "text": "ì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì¤˜. ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ìº¡ì…˜ìœ¼ë¡œ."},
                        {"type": "image_url", "image_url": {"url": f"data:{mime};base64,{b64_img}"}}
                    ]
                )
            ]

            # Vision API í˜¸ì¶œ
            caption = llm.invoke(messages).content.strip()

            # ë§ˆí¬ë‹¤ìš´ êµ¬ì„±
            md_chunks.append(f"![ì´ë¯¸ì§€ {idx + 1}]({img_path.as_posix()})")
            md_chunks.append(f"_í˜ì´ì§€ {page_num + 1}ì—ì„œ ì¶”ì¶œëœ ì´ë¯¸ì§€_")
            md_chunks.append(f"**ìº¡ì…˜:** {caption}\n")

        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨ {pdf_stem} p{page_num + 1} img{idx + 1}: {e}")
            md_chunks.append(f"âš ï¸ ì´ë¯¸ì§€ {idx + 1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

    return md_chunks

def save_markdown_file(markdown_chunks, pdf_path: Path, output_dir: Path):
    """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥"""
    safe_name = "".join(c for c in pdf_path.stem if c.isalnum() or c in (' ', '_')).rstrip()
    md_path = output_dir / f"{safe_name}.md"
    
    with open(md_path, "w", encoding="utf-8") as f:
        f.write("\n".join(markdown_chunks))

def process_pdf_to_markdown(pdf_path: Path, output_dir: Path, images_dir: Path, tables_dir: Path):
    """PDFë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•˜ë©´ì„œ ì´ë¯¸ì§€/í‘œëŠ” ë³„ë„ ì €ì¥"""
    try:
        doc = fitz.open(pdf_path)
        md_chunks = [f"# {pdf_path.stem}"]
        
        for page_num in range(len(doc)):
            page = doc.load_page(page_num)
            
            md_chunks.extend([
                "\n---\n",
                f"## í˜ì´ì§€ {page_num + 1}"
            ])
            
            # í˜ì´ì§€ë¥¼ ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¡œ ë Œë”ë§ (í‘œ í¬ë¡­ìš©)
            pix = page.get_pixmap(dpi=300)
            page_image = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            md_chunks.extend(extract_text_from_page(page))
            
            # í‘œ ì¶”ì¶œ (ë³„ë„ ì €ì¥ + ë§ˆí¬ë‹¤ìš´ì— ì°¸ì¡°)
            md_chunks.extend(extract_tables_from_page(pdf_path, page, page_num, tables_dir, page_image))
            
            # ì´ë¯¸ì§€ ì¶”ì¶œ (ë³„ë„ ì €ì¥ + ë§ˆí¬ë‹¤ìš´ì— ì°¸ì¡°)
            md_chunks.extend(extract_images_from_page(doc, page, page_num, pdf_path.stem, images_dir))
        
        # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥
        save_markdown_file(md_chunks, pdf_path, output_dir)
        doc.close()
        
    except Exception as e:
        logger.error(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨ {pdf_path.name}: {e}")
        raise

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    pdf_root = Path(r"C:\Users\jhwoo\Desktop\SKN_ws\project\SKN13-FINAL-1TEAM\í•œêµ­ë°©ì†¡ê´‘ê³ ì§„í¥ê³µì‚¬\ë‚´ë¶€ë¬¸ì„œ\ì¬ë¬´ì„±ê³¼")
    output_dir = pdf_root / "_markdown_output"
    images_dir = output_dir / "_images"
    tables_dir = output_dir / "_tables"
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir.mkdir(parents=True, exist_ok=True)
    images_dir.mkdir(parents=True, exist_ok=True)
    tables_dir.mkdir(parents=True, exist_ok=True)
    
    # PDF íŒŒì¼ ì°¾ê¸°
    pdf_files = list(pdf_root.rglob("*.pdf"))
    
    if not pdf_files:
        logger.warning(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_root}")
        return
    
    logger.info(f"ì´ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
    
    # ë°°ì¹˜ ì²˜ë¦¬
    success_count = 0
    for pdf_file in tqdm(pdf_files[:3], desc="PDF ë³€í™˜ ì¤‘"): # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 3ê°œë§Œ
        try:
            process_pdf_to_markdown(pdf_file, output_dir, images_dir, tables_dir)
            success_count += 1
        except Exception as e:
            logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨ {pdf_file.name}: {e}")
    
    logger.info(f"ì²˜ë¦¬ ì™„ë£Œ: {success_count}/{len(pdf_files)}ê°œ ì„±ê³µ")
    logger.info(f"ê²°ê³¼ ìœ„ì¹˜:")
    logger.info(f"  - ë§ˆí¬ë‹¤ìš´: {output_dir}")
    logger.info(f"  - ì´ë¯¸ì§€: {images_dir}")
    logger.info(f"  - í‘œ: {tables_dir}")

if __name__ == "__main__":
    main()