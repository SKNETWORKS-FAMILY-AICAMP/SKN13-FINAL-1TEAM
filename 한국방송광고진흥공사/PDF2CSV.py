import os
import pandas as pd
import re
import uuid
import logging
from typing import List, Optional, Tuple, Dict, Any
from dataclasses import dataclass, field
from abc import ABC, abstractmethod

try:
    import fitz  # PyMuPDF
except ImportError:
    print("Error: PyMuPDF is not installed. Please install it using 'pip install PyMuPDF'")
    exit()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class Config:
    CHAPTER_PATTERNS: List[str] = field(default_factory=lambda: [
        r'^ì œ\s*\d+\s*ì¥.*',
        r'^\[\s*ì œ\s*\d+\s*ì¥.*\]',
        r'^Chapter\s+\d+.*',
        r'^\d+\.\s*ì¥.*',
    ])
    ARTICLE_PATTERNS: List[str] = field(default_factory=lambda: [
        r'^(ì œ\s*\d+ì¡°(?:ì˜\d+)?(?:\([^)]*\))?(?:\s*\[[^\]]*\])?)',
        r'^\[\s*(ì œ\s*\d+ì¡°(?:ì˜\d+)?)\s*\]',
        r'^(ì œ\s*\d+\s*ì¡°(?:ì˜\d+)?)',
        r'^(Article\s+\d+)',
        r'^(\d+\.\s*ì¡°)',
    ])
    DATE_PATTERN: str = r'(\d{4})ë…„ë„?\s*(\d{1,2})ì›”'
    OUTPUT_FILENAME: str = 'ì‚¬ë‚´ê·œì •.csv'
    ENCODING: str = 'utf-8-sig'
    MIN_CONTENT_LENGTH: int = 10
    PRESERVE_PARAGRAPH_BREAKS: bool = True



@dataclass
class DocumentChunk:
    """ë¬¸ì„œ ì²­í¬ ë°ì´í„° í´ë˜ìŠ¤"""
    id: str
    filename: str
    last_updated: Optional[str]
    chapter: str
    article: str
    content: str
    paragraph_count: int = 0
    content_type: str = "regulation"  # regulation, header, footer ë“±

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜"""
        return {
            'id': self.id,
            'ì›ë³¸ íŒŒì¼ëª…': self.filename,
            'ìµœì¢… ì—…ë°ì´íŠ¸ ë‚ ì§œ': self.last_updated,
            'ì œ Oì¥': self.chapter,
            'ì œ Oì¡°': self.article,
            'content': self.content,
            'ë‹¨ë½ ìˆ˜': self.paragraph_count,
            'ì½˜í…ì¸  íƒ€ì…': self.content_type
        }

    def is_valid(self) -> bool:
        """ì²­í¬ ìœ íš¨ì„± ê²€ì‚¬"""
        return all([
            self.id,
            self.filename,
            self.article,
            self.content.strip(),
            len(self.content.strip()) >= Config().MIN_CONTENT_LENGTH
        ])

class TextExtractor:
    """PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì „ìš© í´ë˜ìŠ¤"""
    
    def extract_from_pdf(self, file_path: str) -> Optional[str]:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            doc = fitz.open(file_path)
            text = ""
            for page_num, page in enumerate(doc, 1):
                try:
                    page_text = page.get_text()
                    text += page_text
                except Exception as e:
                    logger.warning(f"Failed to extract text from page {page_num}: {e}")
            doc.close()
            return text
        except fitz.FileDataError:
            logger.error(f"Invalid PDF file: {file_path}")
            return None
        except Exception as e:
            logger.error(f"Unexpected error reading PDF {file_path}: {e}")
            return None

class PatternMatcher:
    """íŒ¨í„´ ë§¤ì¹­ ì „ìš© í´ë˜ìŠ¤"""
    
    def __init__(self, config: Config):
        self.config = config
        self.chapter_patterns = [re.compile(pattern) for pattern in config.CHAPTER_PATTERNS]
        self.article_patterns = [re.compile(pattern) for pattern in config.ARTICLE_PATTERNS]
        self.date_pattern = re.compile(config.DATE_PATTERN)
    
    def match_chapter(self, text: str) -> Optional[str]:
        """ì¥ íŒ¨í„´ ë§¤ì¹­"""
        for pattern in self.chapter_patterns:
            match = pattern.match(text)
            if match:
                return text.strip()
        return None
    
    def match_article(self, text: str) -> Optional[Tuple[str, str]]:
        """ì¡° íŒ¨í„´ ë§¤ì¹­ - (ë§¤ì¹­ëœ ì¡° ë²ˆí˜¸, ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸) ë°˜í™˜"""
        for pattern in self.article_patterns:
            match = pattern.match(text)
            if match:
                article_title = match.group(1).strip()
                remaining = text[match.end():].strip()
                return article_title, remaining
        return None
    
    def extract_date_from_filename(self, filename: str) -> Optional[str]:
        """íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ"""
        match = self.date_pattern.search(filename)
        if match:
            year = match.group(1)
            month = match.group(2).zfill(2)
            return f"{year}-{month}"
        return None

class TextProcessor:
    """í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° ì²­í‚¹ ì „ìš© í´ë˜ìŠ¤"""
    
    def __init__(self, config: Config):
        self.config = config
        self.pattern_matcher = PatternMatcher(config)
    
    def preprocess_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ë¼ì¸ ì •ë¦¬ ë° ì •ê·œí™”"""
        lines = text.split('\n')
        processed_lines = []
        
        for line in lines:
            stripped = line.strip()
            if not stripped:
                if self.config.PRESERVE_PARAGRAPH_BREAKS:
                    processed_lines.append('')  # ë¹ˆ ì¤„ ìœ ì§€
                continue
            processed_lines.append(stripped)
        
        return processed_lines
    
    def count_paragraphs(self, content_lines: List[str]) -> int:
        """ë‹¨ë½ ìˆ˜ ê³„ì‚°"""
        if not content_lines:
            return 0
        
        paragraph_count = 1
        for line in content_lines:
            if line == '':  # ë¹ˆ ì¤„ì´ ë‹¨ë½ êµ¬ë¶„ì
                paragraph_count += 1
        
        return paragraph_count
    
    def chunk_text(self, text: str, filename: str) -> List[DocumentChunk]:
        """í…ìŠ¤íŠ¸ë¥¼ ì¥ê³¼ ì¡° ë‹¨ìœ„ë¡œ ì²­í‚¹"""
        chunks = []
        lines = self.preprocess_text(text)
        
        current_chapter = ''
        current_article_title = ''
        current_content = []
        last_updated_date = self.pattern_matcher.extract_date_from_filename(filename)

        for line in lines:
            if not line:  # ë¹ˆ ì¤„ ì²˜ë¦¬
                if current_article_title and self.config.PRESERVE_PARAGRAPH_BREAKS:
                    current_content.append('')
                continue
            
            # ì¥ íŒ¨í„´ í™•ì¸
            chapter_match = self.pattern_matcher.match_chapter(line)
            if chapter_match:
                # ì´ì „ ì¡° ì €ì¥
                if current_article_title:
                    chunk = self._create_chunk(
                        filename, last_updated_date, current_chapter,
                        current_article_title, current_content
                    )
                    if chunk and chunk.is_valid():
                        chunks.append(chunk)
                    current_content = []
                
                current_chapter = chapter_match
                current_article_title = ''
                continue

            # ì¡° íŒ¨í„´ í™•ì¸
            article_match = self.pattern_matcher.match_article(line)
            if article_match:
                article_title, remaining_content = article_match
                
                # ì´ì „ ì¡° ì €ì¥
                if current_article_title:
                    chunk = self._create_chunk(
                        filename, last_updated_date, current_chapter,
                        current_article_title, current_content
                    )
                    if chunk and chunk.is_valid():
                        chunks.append(chunk)
                
                current_article_title = article_title
                current_content = [remaining_content] if remaining_content else []
            else:
                # ì¼ë°˜ ì½˜í…ì¸  ë¼ì¸
                if current_article_title:
                    current_content.append(line)

        # ë§ˆì§€ë§‰ ì¡° ì²˜ë¦¬
        if current_article_title:
            chunk = self._create_chunk(
                filename, last_updated_date, current_chapter,
                current_article_title, current_content
            )
            if chunk and chunk.is_valid():
                chunks.append(chunk)

        return chunks

    def _create_chunk(self, filename: str, last_updated_date: Optional[str],
                      chapter: str, article_title: str, content_lines: List[str]) -> Optional[DocumentChunk]:

        # ğŸ”§ ì¤„ë°”ê¿ˆ ë° í•­ ë³‘í•© ë¦¬íŒ©í† ë§ ì ìš©
        content_lines = self._normalize_paragraph_lines(content_lines)

        content_str = self._join_content_lines(content_lines)
        if not content_str:
            return None

        paragraph_count = self.count_paragraphs(content_lines)

        return DocumentChunk(
            id=str(uuid.uuid4()),
            filename=filename,
            last_updated=last_updated_date,
            chapter=chapter,
            article=article_title,
            content=content_str,
            paragraph_count=paragraph_count
        )

    
    def _join_content_lines(self, lines: List[str]) -> str:
        """ì½˜í…ì¸  ë¼ì¸ë“¤ì„ ì ì ˆíˆ ì¡°ì¸"""
        if not lines:
            return ""
        
        if self.config.PRESERVE_PARAGRAPH_BREAKS:
            # ë¹ˆ ì¤„ì€ ë‹¨ë½ êµ¬ë¶„ìœ¼ë¡œ ìœ ì§€, ì—°ì†ëœ ë¹ˆ ì¤„ì€ í•˜ë‚˜ë¡œ ì •ë¦¬
            result = []
            prev_empty = False
            
            for line in lines:
                if line == '':
                    if not prev_empty:
                        result.append('')
                    prev_empty = True
                else:
                    result.append(line)
                    prev_empty = False
            
            return '\n'.join(result).strip()
        else:
            # ë¹ˆ ì¤„ ì œê±°í•˜ê³  ì¡°ì¸
            return '\n'.join(line for line in lines if line).strip()
        
    def _normalize_paragraph_lines(self, lines: List[str]) -> List[str]:
        """
        ì¤„ë°”ê¿ˆ ë³´ì • ë° í•­ëª© êµ¬ë¶„ (ì˜ˆ: â‘ , â‘¡, â‘¢) ì¸ì‹ ë° ë³‘í•©
        """
        result = []
        buffer = ""
        for line in lines:
            line = line.strip()
            if not line:
                continue

            if re.match(r'^â‘ |â‘¡|â‘¢|â‘£|â‘¤|â‘¥|â‘¦|â‘§|â‘¨|â‘©', line):
                # ì´ì „ buffer ì €ì¥
                if buffer:
                    result.append(buffer.strip())
                buffer = line  # ìƒˆ í•­ ì‹œì‘
            else:
                buffer += " " + line  # ê°™ì€ í•­ëª© ë‚´ ì´ì–´ë¶™ì„

        if buffer:
            result.append(buffer.strip())

        return result


class DocumentSaver(ABC):
    """ë¬¸ì„œ ì €ì¥ ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    def save(self, chunks: List[DocumentChunk], output_path: str) -> bool:
        pass

class CSVDocumentSaver(DocumentSaver):
    """CSV í˜•íƒœë¡œ ë¬¸ì„œ ì €ì¥"""
    
    def __init__(self, encoding: str = 'utf-8-sig'):
        self.encoding = encoding
    
    def save(self, chunks: List[DocumentChunk], output_path: str) -> bool:
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not chunks:
            logger.warning("No chunks to save")
            return False
        
        try:
            # DataFrame ìƒì„±
            data = [chunk.to_dict() for chunk in chunks]
            df = pd.DataFrame(data)
            
            # CSV ì €ì¥
            df.to_csv(output_path, index=False, encoding=self.encoding)
            logger.info(f"Successfully saved {len(chunks)} chunks to '{output_path}'")
            return True
            
        except Exception as e:
            logger.error(f"Failed to save CSV file: {e}")
            return False

class PDFProcessor:
    """PDF ë¬¸ì„œ ì²˜ë¦¬ ë©”ì¸ í´ë˜ìŠ¤ - ê° ì»´í¬ë„ŒíŠ¸ ì¡°ìœ¨ ì—­í• """
    
    def __init__(self, config: Config = None, saver: DocumentSaver = None):
        self.config = config or Config()
        self.text_extractor = TextExtractor()
        self.text_processor = TextProcessor(self.config)
        self.saver = saver or CSVDocumentSaver(self.config.ENCODING)

    def process_pdf_file(self, file_path: str) -> List[DocumentChunk]:
        """ë‹¨ì¼ PDF íŒŒì¼ ì²˜ë¦¬"""
        filename = os.path.basename(file_path)
        logger.info(f"Processing {filename}...")
        
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = self.text_extractor.extract_from_pdf(file_path)
        if not text:
            logger.warning(f"No text extracted from {filename}")
            return []
        
        # 2. í…ìŠ¤íŠ¸ ì²­í‚¹
        chunks = self.text_processor.chunk_text(text, filename)
        logger.info(f"Extracted {len(chunks)} valid chunks from {filename}")
        return chunks

    def process_directory(self, directory: str = None) -> List[DocumentChunk]:
        """ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  PDF íŒŒì¼ ì²˜ë¦¬"""
        if directory is None:
            directory = os.path.dirname(os.path.realpath(__file__))
        
        all_chunks = []
        pdf_files = [f for f in os.listdir(directory) if f.lower().endswith('.pdf')]
        
        if not pdf_files:
            logger.warning(f"No PDF files found in {directory}")
            return []
        
        logger.info(f"Found {len(pdf_files)} PDF files to process")
        
        for filename in pdf_files:
            file_path = os.path.join(directory, filename)
            try:
                chunks = self.process_pdf_file(file_path)
                all_chunks.extend(chunks)
            except Exception as e:
                logger.error(f"Failed to process {filename}: {e}")
        
        logger.info(f"Total chunks extracted: {len(all_chunks)}")
        return all_chunks

    def save_chunks(self, chunks: List[DocumentChunk], output_path: str = None) -> bool:
        """ì²­í¬ë“¤ì„ íŒŒì¼ë¡œ ì €ì¥"""
        if output_path is None:
            output_path = self.config.OUTPUT_FILENAME
        
        return self.saver.save(chunks, output_path)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§• ì˜ˆì‹œ
        config = Config()
        config.PRESERVE_PARAGRAPH_BREAKS = True
        config.MIN_CONTENT_LENGTH = 15
        
        # CSV ì €ì¥ê¸° ì„¤ì •
        csv_saver = CSVDocumentSaver(encoding='utf-8-sig')
        
        # í”„ë¡œì„¸ì„œ ìƒì„±
        processor = PDFProcessor(config=config, saver=csv_saver)
        
        # ì²˜ë¦¬ ì‹¤í–‰
        chunks = processor.process_directory()
        
        if not chunks:
            logger.warning("No data was processed. Exiting.")
            return
        
        # ì €ì¥
        success = processor.save_chunks(chunks)
        if success:
            logger.info("Processing completed successfully!")
            logger.info(f"Average paragraphs per chunk: {sum(c.paragraph_count for c in chunks) / len(chunks):.1f}")
        else:
            logger.error("Failed to save results")
            
    except KeyboardInterrupt:
        logger.info("Process interrupted by user")
    except Exception as e:
        logger.error(f"Unexpected error in main: {e}")

if __name__ == '__main__':
    main()